{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d85610",
   "metadata": {},
   "source": [
    "# Lab no 3 : Pandas, sklearn and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cb5dc",
   "metadata": {},
   "source": [
    "Christophe Thomassin and Cedomir Ljubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2202ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ef8fd",
   "metadata": {},
   "source": [
    "## Q1\n",
    "In the first part, we consider a dataset where the goal is to predict the number of passengers on a\n",
    "given flight\n",
    "What kind of problem is it ? Regression of classification ? Supervised or unsupervised ?\n",
    "\n",
    "> ### **Answer** : \n",
    "> Our model will learn a mapping between features X (features of a given flight) and labels Y (passengers on that flight) on the basis of given data. Therefore the model is **supervised**. Our model makes a numerical prediction and not a class prediction, therefore it is **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c94ebf",
   "metadata": {},
   "source": [
    "## Q2\n",
    "Load the training data from Moodle (train.csv.bz2 ; bz2 is a compression format, pandas can\n",
    "decompress it itself). The target variable is called log_PAX. Do a quick inspection of the dataset.\n",
    "What are the types of the columns ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4af6c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>log_PAX</th>\n",
       "      <th>std_wtd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8897</th>\n",
       "      <td>2011-10-02</td>\n",
       "      <td>DTW</td>\n",
       "      <td>ATL</td>\n",
       "      <td>9.263158</td>\n",
       "      <td>10.427055</td>\n",
       "      <td>7.316967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>2012-09-25</td>\n",
       "      <td>DFW</td>\n",
       "      <td>ORD</td>\n",
       "      <td>12.772727</td>\n",
       "      <td>12.201552</td>\n",
       "      <td>10.641034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8899</th>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>SFO</td>\n",
       "      <td>LAS</td>\n",
       "      <td>11.047619</td>\n",
       "      <td>10.508746</td>\n",
       "      <td>7.908705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>ORD</td>\n",
       "      <td>PHL</td>\n",
       "      <td>6.076923</td>\n",
       "      <td>10.174042</td>\n",
       "      <td>4.030334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8901</th>\n",
       "      <td>2011-11-26</td>\n",
       "      <td>DTW</td>\n",
       "      <td>ATL</td>\n",
       "      <td>9.526316</td>\n",
       "      <td>9.202674</td>\n",
       "      <td>6.167733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DateOfDeparture Departure Arrival  WeeksToDeparture    log_PAX    std_wtd\n",
       "8897      2011-10-02       DTW     ATL          9.263158  10.427055   7.316967\n",
       "8898      2012-09-25       DFW     ORD         12.772727  12.201552  10.641034\n",
       "8899      2012-01-19       SFO     LAS         11.047619  10.508746   7.908705\n",
       "8900      2013-02-03       ORD     PHL          6.076923  10.174042   4.030334\n",
       "8901      2011-11-26       DTW     ATL          9.526316   9.202674   6.167733"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = pd.read_csv(\"train.csv.bz2\")\n",
    "flights.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9f2864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8902 entries, 0 to 8901\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   DateOfDeparture   8902 non-null   object \n",
      " 1   Departure         8902 non-null   object \n",
      " 2   Arrival           8902 non-null   object \n",
      " 3   WeeksToDeparture  8902 non-null   float64\n",
      " 4   log_PAX           8902 non-null   float64\n",
      " 5   std_wtd           8902 non-null   float64\n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 417.4+ KB\n"
     ]
    }
   ],
   "source": [
    "flights.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a15771c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateOfDeparture     0\n",
       "Departure           0\n",
       "Arrival             0\n",
       "WeeksToDeparture    0\n",
       "log_PAX             0\n",
       "std_wtd             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a130ac",
   "metadata": {},
   "source": [
    "> ### **Answer** :\n",
    "> We have  \n",
    ">- Three **numerical** feature columns that are saved as float64 data types.\n",
    ">- Two **categorical** feature columns (deparature, arrival) saved as \"object\" data types. Because they have no ordering we can even say they are **nominal** feature columns (as opposed to ordinal).\n",
    ">- One **date** feature column that is saved as an object data type instead of as datetime. Whether or not dates belong to the group of numerical/quantitative data is a question of semantics. We will transform it into several separate numerical columns in the next question however. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86384c31",
   "metadata": {},
   "source": [
    "## Q3\n",
    "Convert dates to proper dates. Create new integers columns containing respectively the day, the\n",
    "week, the month, the year, a binary variable indicating if this is a work day or holiday (in the US\n",
    "calendar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bec452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[\"DateOfDeparture\"] = pd.to_datetime(\n",
    "    flights[\"DateOfDeparture\"], format=\"%Y/%m/%d\"\n",
    ")  # transform DateOfDeparture in datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34c41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['DayOfDeparture'] = flights[\"DateOfDeparture\"].dt.day\n",
    "flights['WeekOfDeparture'] = flights[\"DateOfDeparture\"].dt.isocalendar().week\n",
    "flights['MonthOfDeparture'] = flights[\"DateOfDeparture\"].dt.month\n",
    "flights['YearOfDeparture'] = flights[\"DateOfDeparture\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa56a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8902 entries, 0 to 8901\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   DateOfDeparture   8902 non-null   datetime64[ns]\n",
      " 1   Departure         8902 non-null   object        \n",
      " 2   Arrival           8902 non-null   object        \n",
      " 3   WeeksToDeparture  8902 non-null   float64       \n",
      " 4   log_PAX           8902 non-null   float64       \n",
      " 5   std_wtd           8902 non-null   float64       \n",
      " 6   DayOfDeparture    8902 non-null   int64         \n",
      " 7   WeekOfDeparture   8902 non-null   UInt32        \n",
      " 8   MonthOfDeparture  8902 non-null   int64         \n",
      " 9   YearOfDeparture   8902 non-null   int64         \n",
      "dtypes: UInt32(1), datetime64[ns](1), float64(3), int64(3), object(2)\n",
      "memory usage: 669.5+ KB\n"
     ]
    }
   ],
   "source": [
    "flights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314aee6",
   "metadata": {},
   "source": [
    "The first four new interger columns have been correctly added. Let's add the holiday column now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a282ca9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>log_PAX</th>\n",
       "      <th>std_wtd</th>\n",
       "      <th>DayOfDeparture</th>\n",
       "      <th>WeekOfDeparture</th>\n",
       "      <th>MonthOfDeparture</th>\n",
       "      <th>YearOfDeparture</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-19</td>\n",
       "      <td>ORD</td>\n",
       "      <td>DFW</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>12.331296</td>\n",
       "      <td>9.812647</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-10</td>\n",
       "      <td>LAS</td>\n",
       "      <td>DEN</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>10.775182</td>\n",
       "      <td>9.466734</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>DEN</td>\n",
       "      <td>LAX</td>\n",
       "      <td>10.863636</td>\n",
       "      <td>11.083177</td>\n",
       "      <td>9.035883</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-10-09</td>\n",
       "      <td>ATL</td>\n",
       "      <td>ORD</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>11.169268</td>\n",
       "      <td>7.990202</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-21</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>11.269364</td>\n",
       "      <td>9.517159</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DateOfDeparture Departure Arrival  WeeksToDeparture    log_PAX   std_wtd  \\\n",
       "0      2012-06-19       ORD     DFW         12.875000  12.331296  9.812647   \n",
       "1      2012-09-10       LAS     DEN         14.285714  10.775182  9.466734   \n",
       "2      2012-10-05       DEN     LAX         10.863636  11.083177  9.035883   \n",
       "3      2011-10-09       ATL     ORD         11.480000  11.169268  7.990202   \n",
       "4      2012-02-21       DEN     SFO         11.450000  11.269364  9.517159   \n",
       "\n",
       "   DayOfDeparture  WeekOfDeparture  MonthOfDeparture  YearOfDeparture  Holiday  \n",
       "0              19               25                 6             2012    False  \n",
       "1              10               37                 9             2012    False  \n",
       "2               5               40                10             2012    False  \n",
       "3               9               40                10             2011    False  \n",
       "4              21                8                 2             2012    False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal = USFederalHolidayCalendar()  # import US holiday calendar\n",
    "holidays = cal.holidays(\n",
    "    start=flights[\"DateOfDeparture\"].min(),\n",
    "    end=flights[\"DateOfDeparture\"].max()\n",
    ").to_pydatetime()  # extract holiday periods as datetime\n",
    "flights[\"Holiday\"] = flights[\"DateOfDeparture\"].isin(holidays).astype(bool)\n",
    "# add binary column Holiday and set 1 if date is in US holidays\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9ae51",
   "metadata": {},
   "source": [
    "We have transformed our 'DateOfDeparture' colummn and we won't be needing it later on so we can drop it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2f9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop(columns=\"DateOfDeparture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84bb97",
   "metadata": {},
   "source": [
    "## Q4\n",
    "First, select numerical features in an automated fashion (not by hand). You can for example use a\n",
    "list comprehension, or flights.select_dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d01b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8902 entries, 0 to 8901\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Departure         8902 non-null   object \n",
      " 1   Arrival           8902 non-null   object \n",
      " 2   WeeksToDeparture  8902 non-null   float64\n",
      " 3   log_PAX           8902 non-null   float64\n",
      " 4   std_wtd           8902 non-null   float64\n",
      " 5   DayOfDeparture    8902 non-null   int64  \n",
      " 6   WeekOfDeparture   8902 non-null   UInt32 \n",
      " 7   MonthOfDeparture  8902 non-null   int64  \n",
      " 8   YearOfDeparture   8902 non-null   int64  \n",
      " 9   Holiday           8902 non-null   bool   \n",
      "dtypes: UInt32(1), bool(1), float64(3), int64(3), object(2)\n",
      "memory usage: 608.7+ KB\n"
     ]
    }
   ],
   "source": [
    "flights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ed197",
   "metadata": {},
   "source": [
    "We can see that the columns 2, 3, 4, 5, 6, 7 and 8 have a numerical data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d227025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8902 entries, 0 to 8901\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   WeeksToDeparture  8902 non-null   float64\n",
      " 1   log_PAX           8902 non-null   float64\n",
      " 2   std_wtd           8902 non-null   float64\n",
      " 3   DayOfDeparture    8902 non-null   int64  \n",
      " 4   WeekOfDeparture   8902 non-null   UInt32 \n",
      " 5   MonthOfDeparture  8902 non-null   int64  \n",
      " 6   YearOfDeparture   8902 non-null   int64  \n",
      "dtypes: UInt32(1), float64(3), int64(3)\n",
      "memory usage: 460.9 KB\n"
     ]
    }
   ],
   "source": [
    "flights_numerical = flights.select_dtypes(\n",
    "    include=['float64', 'int32', 'int64', 'UInt32'])\n",
    "flights_numerical.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d904a",
   "metadata": {},
   "source": [
    "We can verify that out dataframe flights_numerical consists of the exact seven numerical columns in the flights dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f29902",
   "metadata": {},
   "source": [
    "## Q5\n",
    "We will use the Root Mean Squared Error (RMSE) as a figure of merit (performance measure) for\n",
    "this prediction task. Explain how it is defined and why it is relevant here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce7a73",
   "metadata": {},
   "source": [
    "> ### Answer: \n",
    "> We define the RMSE as: \n",
    ">$$ E(y_{pred}(X,w), y) = \\sqrt{ \\sum\\limits_{i = 1}^{N} \\frac{(y_{pred}^{(i)}(X,w) - y^{(i)})^2}{N}} $$ \n",
    "where:  \n",
    ">- $y$ is the vector of the true labels and $y^{(i)}$ is the true value of the i-th example.\n",
    ">- $y_{pred}(X,w)$ is the vector of our model's predictions given the input matrix $X$ and model parameters $w$. \n",
    ">- $y_{pred}^{(i)}(X,w)$ is the model's prediction for the i-th input vector $x^{(i)}$.\n",
    ">- $N$ is the number of input vectors $x$ we consider at a time. It doesn't matter if they are training or test examples.\n",
    "\n",
    ">The RMSE is relevant here, because: \n",
    ">We know that the sum of squares error function $$ E(y_{pred}, y) = \\sum\\limits_{i = 1}^{N} \\frac{1}{2}(y_{pred}^{(i)}(X,w) - y^{(i)})^2$$ has arisen as a consequence of maximizing the likelihood of $y$ under the assumption that $y$ is given by a deterministic function $y_{pred}(x,w)$ with additive gaussian noise $\\epsilon$ so that $$ y = y_{pred}(x,w) + \\epsilon $$\n",
    ">We also know that the argument $y_{pred}^{*}(X,w)$ that minimizes the sum of squares error function (2) also minimizes the RMSE (1).\n",
    ">**Therefore minimizing the RMSE with respect to $y_{pred}$ corresponds to maximizing the likelihood of $y$**.\n",
    "The RMSE is also more relevant than other figures of merit because:\n",
    ">- It takes the mean of the errors, instead of simply the sum of the squared errors, and therefore is useful when comparing the performance on data sets of different sizes.\n",
    ">- The benefit of using RMSE over the MSE is that the metric it produces is in terms of the unit being predicted. However the RMSE decreases the amount of penalty put on very \"wrong\" predictions compared to the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88270e34",
   "metadata": {},
   "source": [
    "## Q6\n",
    "Do a train-test split of the data (a single one, so far. You’ll do K-fold cross validaiton later) and\n",
    "tune the max_depth parameter of a DecisionTreeRegressor. **Explain briefly how this estimator\n",
    "does its prediction**. Plot the RMSE on train and test sets as a function of this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ad87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'log_PAX'\n",
    "X = flights_numerical.drop(columns=[target_name])\n",
    "y = flights_numerical.loc[:, target_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "# Split dataset in a training and a test set (size 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b466c5",
   "metadata": {},
   "source": [
    "> ### Answer:\n",
    "> A \"Decision Tree Regressor\" uses the idea of a \"Decision Tree Classification\" for regression. Instead of a discrete output to predict the cluster of a certain input variable, the model is altered to output a numerical label. In order to make a prediction, the decision tree regressor tries to find a value of a certain feature that minimizes the error that is made when splitting the given data into two subsets at this point. The error used to make the best possible prediction can be choosen. In our case (default) the regressor will try to minimize the mean squared error (MSE). This process is also called maximizing the \"purity\" of the tree.\n",
    "\n",
    ">Let us consider a simple dataset with only one feature $X$ with values $x_i$ and one label $Y$ with values $y_i$. The model will start with deviding the dataset into two subsets according to a certain decision rule. The decision rule has a boundary and every $x_i < x_{bound}$ will be assigned to one subset and every $x_i \\geq x_{bound}$ will be assigned to the other subset. Then, for each subset, we compute the mean label-value for the data in each subset and assign this value as prediction $\\hat{y}$ to every $x_i$ in the according subset. Based on this computation the MSE of the split can be computeted according to the formula above. To choose which decision rule to choose, the model will derive the MSE for every possible split, where possible split values are the mean of adjacent values ($x_i$ have to be sorted by increasing value). From there on, the decision rule resulting into the minimum MSE will be adapted. \n",
    "\n",
    ">In case we have multiple features (as in our case) the computation of the MSE no only be conducted for all possible split values within a certain feature, but also for all existing features. Once again, we will choose to adapt the split value of the feature that allows us to minimize the MSE.\n",
    "\n",
    "> During training/fitting the Decision Tree Regressor will build a \"tree\" of nodes and leaf. According to the framework, which we just introduced, the model will iteratively apply decision rules on the dataset, where each node describes the application of such a decision rule. A leaf corresponds to the prediction that is made when for a subset if there are no further nodes applied on the set.  \n",
    "\n",
    "> The prediction for a new input $x_{n+1}$ is made by \"running\" the input through the nodes that were created during the training phase. Once it reaches a leaf, the prediction associated with the leaf will be made for the input value $x_{n+1}$. The max depth represents the maximum number of nodes a new example will go through before it comes to a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28d9bb",
   "metadata": {},
   "source": [
    "We want to pick the max depth which gives the lowest generalization error while still having low bias, therefore the following function will highlight the max depth which gives **lowest RMSE on the test data**. Technically, we shouldn't call it test set but rather validation set, because we are tuning our models hyperparameters to it, but for now we keep the notation test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3351730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_errors_plot(X_train, X_test, y_train, y_test, depth_range=50):\n",
    "    \"\"\"\n",
    "    Computes and plots the train and test error (RMSE) for range of max depths.\n",
    "    Args:\n",
    "      X_train (ndarray (n, d)): Training data, m examples, d variables \n",
    "      y_train (ndarray (n,)): target values\n",
    "      X_test (ndarray (m, d)): Test data, m examples, d variables \n",
    "      y_test (ndarray (m,)): target values\n",
    "      depth_range (scalar): Number of max depths to test \n",
    "    Returns:\n",
    "      Plot of the train and test errors\n",
    "    \"\"\"\n",
    "    testerror = np.zeros(depth_range)\n",
    "    trainerror = np.zeros(depth_range)\n",
    "    x = np.arange(1, depth_range + 1)\n",
    "\n",
    "    for i in range(depth_range):\n",
    "        regressor = DecisionTreeRegressor(random_state=1, max_depth=i+1)\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        pred_test = regressor.predict(X_test)\n",
    "        pred_train = regressor.predict(X_train)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))\n",
    "        testerror[i] = rmse_test\n",
    "        trainerror[i] = rmse_train\n",
    "\n",
    "    optimal_maxdepth = np.argmin(testerror) + 1\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.set_title(\n",
    "        \"Find optimal max depth for decision tree regression\", fontsize=12)\n",
    "    ax.set_xlabel(\"Max depth\")\n",
    "    ax.set_ylabel(\"Error\")\n",
    "\n",
    "    ax.plot(x, testerror, marker='o',\n",
    "            color=\"orangered\", label=\"test error\", lw=1)\n",
    "    ax.plot(x, trainerror, marker='o', color=\"blue\", label=\"train error\", lw=1)\n",
    "    ax.axvline(optimal_maxdepth, lw=1, color='g')\n",
    "    ax.annotate(\"optimal max depth\", xy=(optimal_maxdepth, 0.8), xytext=(0.2, 0.4), \n",
    "                textcoords='axes fraction', fontsize=10,\n",
    "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", \n",
    "                                color='r', lw=1))\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1ed6e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparam_errors_plot(X_train, X_test, y_train, y_test, depth_range=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa73077",
   "metadata": {},
   "source": [
    "The train error decreases with the max depth and even seems to converge to 0. The test error decreases initially and then increases again. This is a clear sign of overfitting: as the max depth gets larger, the bias reduces and the variance increases. As said before we pick the one that minimizes the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875c373",
   "metadata": {},
   "source": [
    "## Q7\n",
    "Test the impact of using or not a StandardScaler on the features, for this estimator with the found\n",
    "value of max_depth (use a Pipeline). Explain the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f0952",
   "metadata": {},
   "source": [
    "We test the impact both on the train and set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae0a017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The respective errors on the test set are:\n",
      "rmse_test_scaled = 0.9476437400032037\n",
      "rmse_test_normal = 0.9476437400032037\n",
      "The respective errors on the train set are:\n",
      "rmse_train_scaled = 0.8642916513584539\n",
      "rmse_train_normal = 0.8642916513584538\n"
     ]
    }
   ],
   "source": [
    "scaled_regressor = make_pipeline(\n",
    "    StandardScaler(), DecisionTreeRegressor(random_state=1, max_depth=8))\n",
    "regressor = DecisionTreeRegressor(random_state=1, max_depth=8)\n",
    "\n",
    "# Fitting the two regressors\n",
    "regressor.fit(X_train, y_train)\n",
    "scaled_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting for test data\n",
    "test_pred_scaled = scaled_regressor.predict(X_test)\n",
    "test_pred_normal = regressor.predict(X_test)\n",
    "\n",
    "# Predicting for train data\n",
    "train_pred_scaled = scaled_regressor.predict(X_train)\n",
    "train_pred_normal = regressor.predict(X_train)\n",
    "\n",
    "# Calculating the RMSE for test predictions\n",
    "rmse_test_scaled = np.sqrt(mean_squared_error(y_test, test_pred_scaled))\n",
    "rmse_test_normal = np.sqrt(mean_squared_error(y_test, test_pred_normal))\n",
    "\n",
    "# Calculating the RMSE for train predictions\n",
    "rmse_train_scaled = np.sqrt(mean_squared_error(y_train, train_pred_scaled))\n",
    "rmse_train_normal = np.sqrt(mean_squared_error(y_train, train_pred_normal))\n",
    "\n",
    "print(\"The respective errors on the test set are:\")\n",
    "print(f\"{rmse_test_scaled = }\")\n",
    "print(f\"{rmse_test_normal = }\")\n",
    "print(\"The respective errors on the train set are:\")\n",
    "print(f\"{rmse_train_scaled = }\")\n",
    "print(f\"{rmse_train_normal = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9611a",
   "metadata": {},
   "source": [
    "**Answer Question 7** \n",
    "> There is no difference in the RMSE of the scaled and the non scaled model. That is because **no feature scaling is required when working with tree based models**. These models are actually invariant to all monotonic transformations of the features/inputs, such as the log transformation for example or in our case the standard scaling (which is monotonic). The phenomenon seemt intuitively, because decision trees split data into multiple groups and the splits depend only on the relative position of the features. For example without scaling we might split if feature $x_1 > 100$ and with scaling if $x_1 > 1$.\n",
    "\n",
    "> **Therefore we get the same result with and without scaled data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518062eb",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "For a LinearRegression model with fit_intercept=True, test the impact of using a\n",
    "StandardScaler. Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28730089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The respective errors on the test set are:\n",
      "LR_test_rmse = 0.9885140213757009\n",
      "LRS_test_rmse = 0.9885140213756995\n"
     ]
    }
   ],
   "source": [
    "# LR stands for \"Linear Regression\" and the S for \"scaled\"\n",
    "LR = LinearRegression(fit_intercept=True)\n",
    "LRS = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=True))\n",
    "\n",
    "LR.fit(X_train, y_train)\n",
    "LRS.fit(X_train, y_train)\n",
    "\n",
    "LR_testpred = LR.predict(X_test)\n",
    "LRS_testpred = LRS.predict(X_test)\n",
    "\n",
    "LR_test_rmse = np.sqrt(mean_squared_error(y_test, LR_testpred))\n",
    "LRS_test_rmse = np.sqrt(mean_squared_error(y_test, LRS_testpred))\n",
    "\n",
    "print(\"The respective errors on the test set are:\")\n",
    "print(f\"{LR_test_rmse = }\")\n",
    "print(f\"{LRS_test_rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0afd6",
   "metadata": {},
   "source": [
    "> ### **Answer:**\n",
    "> We come across yet another interesting mathematical result which is that the linear regression with an intercept term is invariant to the standardization of its features, when adapting a maximum likelihood approach to determine the parameters w (meaning without regularization). \n",
    "> Let's prove it by showing that linear regression is invariant to centering and to scaling of it's features X, when taking the maximum likelihood approach.\n",
    "\n",
    "> As a reminder, in the frequentist approach our goal is to minimize ${\\|y - Xw\\|}$ with respect to $w$, or equivalently ${\\|y - Xw - w_0\\|}$ if we don't have an extra incercept column in X.\n",
    "\n",
    ">  **Scaling**: We know that the result of this minimization problem $y_{pred}^{*} = Xw^{*}$ is the orthogonal projection of the label vector $y$ onto the subspace spanned by the column vectors of $X$. By the definition of a span, scaling the vectors that make up the span doesn't change the subspace that it generates. Therefore our prediction $y_{pred}$ is invariant to the scaling of the columns of X. \n",
    "\n",
    ">  **Centering**: If we have an input matrix $X$, with $d$ columns denoted as $X^{[i]}$, a parameter vector $w = (w_1, ..., w_d)^T$, an intercept vector $w_0$ and $d$ \"centering\" vectors $\\lambda_i$ (where each element is equal to the mean of the corresponding $X^{[i]}$ column) then we can write our prediction vector $y_{pred}$  as:\n",
    "$$ y_{pred} = w_0 + w_1(X^{[1]} - \\lambda_1) + ... w_d(X^{[d]} - \\lambda_d)$$\n",
    "which can be rewritten as:\n",
    "$$ y_{pred} = (w_0 - \\sum\\limits_{i = 1}^{d} w_i\\lambda_i) + w_1X^{[1]} + w_dX^{[d]}$$\n",
    "\n",
    "> The minimization problem for **centered features X** can we written as: $${\\|y - (X - J\\lambda)w - w_0 \\|}$$, where $J$ is a matrix of only ones, $\\lambda$ is a vector where each element $\\lambda_i$ represents the mean of the column $X^{[i]}$.\n",
    "> And thanks to the result above, we see that the minimization problem can be rewritten as: $${\\|y - Xw - \\beta_0\\|}$$\n",
    "\n",
    "> where the intercept $\\beta_0 =  w_0 - \\sum\\limits_{i = 1}^{d} w_i\\lambda_i$\n",
    "\n",
    "> **Therefore we see that the two regression problems, one with and one without centering of features, are identical apart from the intercept term when taking the MLE approach.** The optimal parameters $w^{*} = (w_1^{*}, ..., w_d^{*})^T$ for both regression problems will even be the same, the only difference will be in the intercept terms of the models, but we even know the relationship between them!  \n",
    "> As the parameters $(w_1^{*}, ..., w_d^{*})$ for both regression problems are the same, regularized linear rigression will also be invariant to centering of the features, as long as we don't regularize the intercept, but will not be invariant to scaling.\n",
    "\n",
    "> $QED$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d843c",
   "metadata": {},
   "source": [
    "## Q9\n",
    "Now, we use again the full dataset. We will encode the categorical features with a OneHotEncoder.  \n",
    "Create a one hot encoder instance, fit it on the data, transform the data and display all categories\n",
    "inferred by the transformer. Delete the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a2e082",
   "metadata": {},
   "source": [
    "First we select the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f372e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8897</th>\n",
       "      <td>DTW</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>DFW</td>\n",
       "      <td>ORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8899</th>\n",
       "      <td>SFO</td>\n",
       "      <td>LAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>ORD</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8901</th>\n",
       "      <td>DTW</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Departure Arrival\n",
       "8897       DTW     ATL\n",
       "8898       DFW     ORD\n",
       "8899       SFO     LAS\n",
       "8900       ORD     PHL\n",
       "8901       DTW     ATL"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_obj = flights.select_dtypes(include=['object'])\n",
    "flights_obj.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251e13c",
   "metadata": {},
   "source": [
    "We create the one hot encoder instance and we fit and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81597819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='error', sparse=True)\n",
    "categorical_onehot = enc.fit_transform(flights_obj).toarray()\n",
    "# Use OneHotEncoder to encode categorical features\n",
    "categorical_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402cfa6",
   "metadata": {},
   "source": [
    "We display the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f8ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['ATL', 'BOS', 'CLT', 'DEN', 'DFW', 'DTW', 'EWR', 'IAH', 'JFK',\n",
       "        'LAS', 'LAX', 'LGA', 'MCO', 'MIA', 'MSP', 'ORD', 'PHL', 'PHX',\n",
       "        'SEA', 'SFO'], dtype=object),\n",
       " array(['ATL', 'BOS', 'CLT', 'DEN', 'DFW', 'DTW', 'EWR', 'IAH', 'JFK',\n",
       "        'LAS', 'LAX', 'LGA', 'MCO', 'MIA', 'MSP', 'ORD', 'PHL', 'PHX',\n",
       "        'SEA', 'SFO'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(enc.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12e9d4",
   "metadata": {},
   "source": [
    "Now we delete the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d2b9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "del categorical_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24683e70",
   "metadata": {},
   "source": [
    "## Q10\n",
    "Create a Pipeline standardizing the numerical features, and one-hot encoding categorical features,\n",
    "followed by the application of a RandomForestRegressor to the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60163dea",
   "metadata": {},
   "source": [
    "There might be a more automated way of determining which column names correspond to which category, but we will go for the lazy way as was done in the tutorial notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7aa4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_columns = ['WeeksToDeparture', 'std_wtd',\n",
    "                   'DayOfDeparture', 'WeekOfDeparture',\n",
    "                   'MonthOfDeparture', 'YearOfDeparture']\n",
    "one_hot_encoding_columns = ['Departure', 'Arrival']\n",
    "binary_encoding_columns = [\"Holiday\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5220c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='error', sparse=True),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)\n",
    "])  # Create preprocessor with all Encoders and the StandardScaler\n",
    "forest_processed = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestRegressor(random_state=1)\n",
    ")  # Make pipeline out of preprocessor and RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9bd7f",
   "metadata": {},
   "source": [
    "## Q11\n",
    "Perform grid-search on the cross-validation error to tune simultaneously the n_estimators and\n",
    "max_depth of the prediction step of your pipeline. Comment on the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5914a6",
   "metadata": {},
   "source": [
    "We redefine our training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786072d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>WeeksToDeparture</th>\n",
       "      <th>std_wtd</th>\n",
       "      <th>DayOfDeparture</th>\n",
       "      <th>WeekOfDeparture</th>\n",
       "      <th>MonthOfDeparture</th>\n",
       "      <th>YearOfDeparture</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD</td>\n",
       "      <td>DFW</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>9.812647</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAS</td>\n",
       "      <td>DEN</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>9.466734</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEN</td>\n",
       "      <td>LAX</td>\n",
       "      <td>10.863636</td>\n",
       "      <td>9.035883</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL</td>\n",
       "      <td>ORD</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>7.990202</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>9.517159</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Departure Arrival  WeeksToDeparture   std_wtd  DayOfDeparture  \\\n",
       "0       ORD     DFW         12.875000  9.812647              19   \n",
       "1       LAS     DEN         14.285714  9.466734              10   \n",
       "2       DEN     LAX         10.863636  9.035883               5   \n",
       "3       ATL     ORD         11.480000  7.990202               9   \n",
       "4       DEN     SFO         11.450000  9.517159              21   \n",
       "\n",
       "   WeekOfDeparture  MonthOfDeparture  YearOfDeparture  Holiday  \n",
       "0               25                 6             2012    False  \n",
       "1               37                 9             2012    False  \n",
       "2               40                10             2012    False  \n",
       "3               40                10             2011    False  \n",
       "4                8                 2             2012    False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = 'log_PAX'\n",
    "X = flights.drop(columns=[target_name])\n",
    "y = flights.loc[:, target_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8abda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"The hyper-parameters for a preprocessed random forest model are:\")\n",
    "# for param_name in forest_processed.get_params().keys():\n",
    "#    print(param_name)\n",
    "# We commented this out because we don't want it to take up too much space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "param_grid = {'randomforestregressor__n_estimators': (np.arange(1, 102, 20)),\n",
    "              'randomforestregressor__max_depth': (np.arange(1, 102, 20))}\n",
    "\n",
    "forest_grid = GridSearchCV(forest_processed,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           n_jobs=12, cv=5)\n",
    "\n",
    "start = time.time()\n",
    "forest_grid.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The RMSE using a {forest_grid.__class__.__name__} model is \"\n",
    "    f\"{- forest_grid.score(X_train, y_train):.2f} on the train set\"\n",
    "    f\" and {- forest_grid.score(X_test, y_test):.2f} on the test set.\"\n",
    "    f\" The optimal hyperparams were found in {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733a607",
   "metadata": {},
   "source": [
    "> ### **Answer:**\n",
    "> Gridsearch CV is computationally very expensive because it requires a number of training\n",
    "runs that can be exponential in the number of parameters. In our case we have two parameters with each 6 choices, so we train $6^2$ models, without even considering the Kfold cross validation. Kfold cross validation then increases the number of training runs necessary by a factor of K (cv). So in our case that gives $6*6*5 = 180$ training runs in order to find the optimal hyperparameters. By increasing the number of n_jobs we speed up the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52a26a",
   "metadata": {},
   "source": [
    "## Q12\n",
    "> Get the estimator with the best params. Save both the full pipeline and the best model to disk\n",
    "with joblib. Load them from disk. Why is the ability to dump estimators useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c505e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: {forest_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(forest_processed, 'forest_processed_joblib')\n",
    "joblib.dump(forest_grid, 'model_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c00a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = joblib.load(\"model_best\")\n",
    "forest_proceddes_joblib = joblib.load(\"forest_processed_joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f975e",
   "metadata": {},
   "source": [
    "> ## Answer:\n",
    "> As machine learning gets more and more advanced, pipelines are becoming very complex and enhanced. Thus, building and using demand a lot of computation time and power. To prevent costly recomputation of a pipeline during the creation or testing, we can use the \"dump\" and \"load\" functionalities of joblib. These functions allow us to respectively save and reload python objects to/from a PC's disk. Therefore, we do not need to rerun a model e.g., finish training our parameters, but can simply load it from a file. Furthermore joblib can load and save objects with numpy arrays faster than Pickle, python's built-in method for saving and loading. This can be especially helpful for algorithms for which we need to store whole dataset or have a lot of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044882b",
   "metadata": {},
   "source": [
    " ## Q13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ddc47",
   "metadata": {},
   "source": [
    "What is the cost of fitting a KNN ? and of predicting for one new point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e331d1",
   "metadata": {},
   "source": [
    "> ### **Answer:**\n",
    "> Let us introduce n as the number of points in the training dataset, d as the number of features and k as the number of neighbours that we consider for allocating a new point to a certain cluster. \n",
    "For every new point we calculate the distance from the new point to every other point in the training data, then \"sort\" the distances in a ascending order and consider the k-nearest points to do a majority-vote. It is not necessary to sort the distances array, there are also less computationally expensive algorithms that determine the indexes of the k smallest elements of the array without sorting. \n",
    "\n",
    ">This model has no parameters, only hyperparameters such as k and perhaps the distance function used, thus this training needs no fitting.\n",
    "\n",
    ">The complexity of the fitting is a constant or O(1) as we just have to save the data.\n",
    "\n",
    ">**The most optimal algorithm can make a prediction for one new point in O(nd) operations.** That all depends on the sorting algorithm used in the last step.\n",
    ">- Each distance computation requires a runtime of O(d). \n",
    ">- We have to compute the distance from the new point n times, which results into a runtime of O(nd). \n",
    ">- For the last step we can find the k-th smallest element of the array in O(n) computations using the algorithm from this source:\n",
    "https://www.geeksforgeeks.org/kth-smallestlargest-element-unsorted-array-set-3-worst-case-linear-time/\n",
    "Once we have the k-th smallest element we can select the remaining k-1 elements in O(n) computations. Therefore selecting the k smallest indexes of the array takes O(n) operations. \n",
    "\n",
    ">Overall we get a cost of predicting a new point of O(nd + n) = O(nd) if we use the most efficient algorithms for the last step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41229d",
   "metadata": {},
   "source": [
    " ## Q14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48c137",
   "metadata": {},
   "source": [
    "Implement a KNearestNeighbor class with __init__, fit and predict. scipy.stats.mode may\n",
    "be useful for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed97853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbours(BaseEstimator):  # , ClassifierMixin):\n",
    "    \"\"\"KNearestNeighbor classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the KNearestNeighbor classifier from the training dataset.\n",
    "\n",
    "        Args:\n",
    "            X (np array of shape (n_samples, n_features)): Training data.\n",
    "            y (np array of shape (n_samples, 1)): Target values.\n",
    "        Returns:\n",
    "            self : KNearestNeighbour\n",
    "                The fitted KNearestNeighbour classifier.\n",
    "        Errors:\n",
    "            ValueError : if sizes of X and y don't match.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Input validation for standard estimators\n",
    "        check_classification_targets(y)\n",
    "        # Input validation classification target variables\n",
    "        self.classes_ = np.unique(y)\n",
    "        # Create classed for ever modality in y\n",
    "        self.X_train_ = X\n",
    "        self.y_train_ = y\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.n_train_ = X.shape[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the provided data.\n",
    "\n",
    "        Args:\n",
    "            X (np array of shape (n_samples, n_features)): Test samples.\n",
    "        Returns:\n",
    "            index_closest (ndarray of shape (n_samples,)): Class labels for\n",
    "                each sample.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        distances = np.zeros((X.shape[0], self.n_train_))\n",
    "        k_nearest_indexes = np.zeros((X.shape[0], self.k))\n",
    "        k_nearest_labels = np.zeros((X.shape[0], self.k))\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self.n_train_):\n",
    "                distance = np.linalg.norm(X[i, :] - self.X_train_[j, :])\n",
    "                distances[i, j] = distance\n",
    "            k_nearest_indexes[i, :] = np.argsort(distances[i, :])[:self.k]\n",
    "            k_nearest_labels[i, :] = self.y_train_[\n",
    "                k_nearest_indexes[i, :].astype(int)]\n",
    "\n",
    "        y_pred = stats.mode(k_nearest_labels, axis=1)\n",
    "        return y_pred[0]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Return the average number of samples corectly classified.\n",
    "\n",
    "        Args:\n",
    "            X (np array of shape (n_samples, n_features)): Test or train samples.\n",
    "            y (np array of shape (n_samples,)): Class for each sample.\n",
    "        Returns:\n",
    "            score (_type_): average number of samples corectly classified\n",
    "        Errors:\n",
    "            ValueError : if sizes of X and y don't match.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        accuracy_score = np.mean(y == self.predict(X))\n",
    "        return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b35120",
   "metadata": {},
   "source": [
    "## Q15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a20c1",
   "metadata": {},
   "source": [
    "Generate data with the function rand_checkers on Moodle. Describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rand_checkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X, y] = rand_checkers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a35e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num = len(np.unique(y_train))\n",
    "print(f\"There are {cat_num} different categories in y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ffbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "# Loop Over Unique Categories in Y\n",
    "for num, cat in enumerate(np.unique(y_train)):\n",
    "    # Get indices for when Y is equal to cat\n",
    "    ind = np.where(y_train == cat)\n",
    "    # Plot the data for that category only & assign a label\n",
    "    plt.scatter(X_train[ind, 0], X_train[ind, 1], c=colors[num],\n",
    "                label='Category {0}'.format(cat))\n",
    "    # Define labels\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "_ = plt.legend(loc=(1.04, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e7670",
   "metadata": {},
   "source": [
    "> ## **Answer:**\n",
    "> We can see a dataset with two different features $x_1$ and $x_2$. Each datapoint $x_i$, determined by its values $x_{1,i}$ and $x_{2,i}$, is assigned to one of three categories. The categories have been displayed using three different colours. On first sight, we can not identify any rule that seems to link the category of a data point with its feature-values. Although, we can form hypotheses by examining areas in which one category seems to dominate the number of available data points. For example, for $x_1 \\in [-2, 0]$ and $x_2 \\in [-2, 0]$ category 1 (red) clearly seems to be the most probable category for new data samples within this area. Thus, we are tempted to a assign a new data point $(x_{1,n+1} = 0, x_{2,n+1} = 0)$ to category 1. Nevertheless, as we can see observing the area $x_1 \\in [0, 2]$ and $x_2 \\in [-1, 1]$, we can not determine a fast-forward decision rule for all input values of our problem. Therefore, we should look for a reliable framework to categorize new value throughout our entire definition range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104734f9",
   "metadata": {},
   "source": [
    "## Q16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04969fb",
   "metadata": {},
   "source": [
    "Use 10 fold cross validation to tune the parameter K of your estimator on this dataset (it may\n",
    "help to have your having your class inherit from BaseEstimator and ClassifierMixin, that can\n",
    "be imported from sklearn.base). Plot the average loss on the train and test sets as a function of\n",
    "K. Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b142cef",
   "metadata": {},
   "source": [
    "**For the loss function we will go with the misclassification rate**, so the percentage of classifications that we got wrong. We will therefore compute the accuracies and then in the final step compute the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de14f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_knn(X, y, max_k):\n",
    "    \"\"\"Compute and plot the average train and test errors \n",
    "    for 10-fold cross validation.\n",
    "    Args:\n",
    "        X (np array of shape (n_samples, n_features)): All the data (train and test)\n",
    "        y (np array of shape (n_samples, 1)): Target values.\n",
    "    Returns:\n",
    "        Average accuracies on training\n",
    "        Average accuracies on test\n",
    "        Plot of avarage misclassification\n",
    "    \"\"\"\n",
    "    # This array will hold all the avarages\n",
    "    accu_train_avg = []\n",
    "    accu_test_avg = []\n",
    "    for k in range(1, max_k, 1):\n",
    "\n",
    "        strtfdKFold = StratifiedKFold(n_splits=10)\n",
    "        kfold = strtfdKFold.split(X, y)\n",
    "        knn = KNearestNeighbours(k=k)\n",
    "\n",
    "        # This array will hold the results for each fold of k-fold CV\n",
    "        accuracies_kfold_train = []\n",
    "        accuracies_kfold_test = []\n",
    "\n",
    "        for i, (train, test) in enumerate(kfold):\n",
    "\n",
    "            knn.fit(X[train, :], y[train])\n",
    "            accuracies_kfold_train.append(knn.score(X[train, :], y[train]))\n",
    "            accuracies_kfold_test.append(knn.score(X[test, :], y[test]))\n",
    "\n",
    "        single_train_avg = round(np.mean(accuracies_kfold_train), 2)\n",
    "        single_test_avg = round(np.mean(accuracies_kfold_test), 2)\n",
    "\n",
    "        accu_train_avg.append(single_train_avg)\n",
    "        accu_test_avg.append(single_test_avg)\n",
    "\n",
    "    # print(accu_train_avg)\n",
    "    # print(accu_train_avg)\n",
    "\n",
    "    x = np.arange(1, max_k, 1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 8))\n",
    "    plt.style.use('bmh')\n",
    "    ax.set_title(\n",
    "        \"Misclassification of KNN algo for different params of k\", fontsize=14)\n",
    "    ax.set_xlabel(\"k\")\n",
    "    ax.set_ylabel(\"Misclassification\")\n",
    "    ax.plot(x, 1 - np.array(accu_train_avg),\n",
    "            marker='o', label='Avg train loss')\n",
    "    ax.plot(x, 1 - np.array(accu_test_avg), marker='o', label='Avg test loss')\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5678346",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_knn(X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b0f6d",
   "metadata": {},
   "source": [
    "## Comment Q16:\n",
    "The training loss starts at 0, because for 1NN the algo picks the prediction of the train input itself. \n",
    "Values of K larger than 10 seem very inefficient and we see that both the train and test loss goes up.  \n",
    "\n",
    "For values 1 to 10 there is a general upwards trend in the loss function for the train set, but there is no clear pattern for the test set. This may be due to the size of our dataset, that only contains 192 values.\n",
    "\n",
    "Optimal values for k seem to be 1, 5 or 7. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
